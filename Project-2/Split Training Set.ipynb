{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "training_file = \"data/training.zh-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44016\n"
     ]
    }
   ],
   "source": [
    "# Loads the paired sentences from the data file\n",
    "\n",
    "with open(training_file) as f:\n",
    "    paired_sentences = f.read().splitlines()\n",
    "    \n",
    "print(len(paired_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum target sentence length: 65\n",
      "['\"', 'y', '\"', 'as', 'in', 'yokohama', ',', '\"', 'a', '\"', 'as', 'in', 'america', ',', '\"', 'm', '\"', 'as', 'in', 'mexico', ',', '\"', 'a', '\"', 'as', 'in', 'america', ',', '\"', 'g', '\"', 'as', 'in', 'germany', ',', '\"', 'u', '\"', 'as', 'in', 'union', ',', '\"', 'c', '\"', 'as', 'in', 'china', ',', '\"', 'h', '\"', 'as', 'in', 'hong', 'kong', ',', 'and', '\"', 'i', '\"', 'as', 'in', 'italy', '.']\n",
      "\n",
      "Number of excluded sentence pairs given target sentence length constraint: 63\n",
      "Number of remaining sentence pairs given target sentence length constraint: 43953\n"
     ]
    }
   ],
   "source": [
    "# Calculating longest sentence size for target language\n",
    "# Calculating number of target sentences with length greater than a certain value\n",
    "\n",
    "target_sentence_size_value_constraint = 30 #INCLUDING\n",
    "\n",
    "max_ = 0\n",
    "count = 0\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    english_side = pair_sentence[1].split(' ')\n",
    "    if (len(english_side) > max_):\n",
    "        max_ = len(english_side)\n",
    "        max_sentence = english_side\n",
    "    if (len(english_side) > target_sentence_size_value_constraint):\n",
    "        count = count + 1\n",
    "        \n",
    "print(\"Maximum target sentence length: \" + str(max_))\n",
    "print(max_sentence)\n",
    "print(\"\\n\" + \"Number of excluded sentence pairs given target sentence length constraint: \" + str(count))\n",
    "print(\"Number of remaining sentence pairs given target sentence length constraint: \" + \n",
    "      str(len(paired_sentences) - count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of excluded sentence pairs given target sentence length constraint: 43953\n"
     ]
    }
   ],
   "source": [
    "# Selects only those paired sentences whose target sentence length\n",
    "# is smaller than the specified length constraint\n",
    "\n",
    "target_size_constrained_paired_sentences = []\n",
    "\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    english_side = pair_sentence[1].split(' ')\n",
    "    if (len(english_side) <= target_sentence_size_value_constraint):\n",
    "        target_size_constrained_paired_sentences.append(pair)\n",
    "        \n",
    "print(\"Number of excluded sentence pairs given target sentence length constraint: \" +\n",
    "      str(len(target_size_constrained_paired_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determines the partition limits of the training data\n",
    "# for dividing the original data file into 3 similar parts\n",
    "\n",
    "\n",
    "number_of_training_examples = len(target_size_constrained_paired_sentences)\n",
    "\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    number_of_subset_training_examples = number_of_training_examples/3\n",
    "else:\n",
    "    number_of_subset_training_examples12 = number_of_training_examples/3\n",
    "    number_of_subset_training_examples3 = number_of_training_examples - 2*number_of_subset_training_examples12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates 3 new training data files, which are\n",
    "# basically disjoint sets of the original file\n",
    "# and where their union is the entire training data (on constrained length)\n",
    "\n",
    "\n",
    "low_limit = 0\n",
    "\n",
    "# First Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = number_of_subset_training_examples12\n",
    "\n",
    "f = open('data/training_subset1_size' + str(target_sentence_size_value_constraint) + '.zh-en', 'w')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(target_size_constrained_paired_sentences[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Second Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = 2*number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = 2*number_of_subset_training_examples12\n",
    "\n",
    "f = open('data/training_subset2_size' + str(target_sentence_size_value_constraint) + '.zh-en', 'w')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(target_size_constrained_paired_sentences[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Third Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = 3*number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = 2*number_of_subset_training_examples12 + number_of_subset_training_examples3\n",
    "\n",
    "f = open('data/training_subset3_size' + str(target_sentence_size_value_constraint) + '.zh-en', 'w')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(target_size_constrained_paired_sentences[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
