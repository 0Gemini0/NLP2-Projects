{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import operator\n",
    "from __future__ import print_function\n",
    "import random\n",
    "\n",
    "training_file = \"data/training.zh-en\"\n",
    "lexicon_file = \"lexicon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_n = 4 #INCLUDING\n",
    "top_n_NULL = 3 #INCLUDING\n",
    "sentence_size_value_constraint = 10 #INCLUDING\n",
    "percentage_of_one_occurence_words_to_UNK = 0.8\n",
    "UNK = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads the paired sentences from the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44016\n"
     ]
    }
   ],
   "source": [
    "with open(training_file, encoding='utf8') as f:\n",
    "    paired_sentences = f.read().splitlines()\n",
    "    \n",
    "print(len(paired_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "汗牛充栋\n"
     ]
    }
   ],
   "source": [
    "source_TF = {}\n",
    "\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    chinese_side = pair_sentence[0].split(' ')\n",
    "    for source_word in chinese_side:\n",
    "        if source_word not in source_TF:\n",
    "            source_TF[source_word] = 1\n",
    "        else:\n",
    "            source_TF[source_word] += 1\n",
    "\n",
    "one_word_occurrences = []\n",
    "\n",
    "for key in source_TF:\n",
    "    if source_TF[key] == 1:\n",
    "        one_word_occurrences.append(key)\n",
    "\n",
    "#print(len(list(source_TF.keys())))\n",
    "number_of_one_word_occurrences = len(one_word_occurrences)\n",
    "#print(number_of_one_word_occurrences)\n",
    "number_of_words_mapped_to_UNK = int(number_of_one_word_occurrences*percentage_of_one_occurence_words_to_UNK)\n",
    "#print(number_of_words_mapped_to_UNK)\n",
    "\n",
    "random.shuffle(one_word_occurrences)\n",
    "#print(one_word_occurrences)\n",
    "\n",
    "words_mapped_to_UNK = one_word_occurrences[:number_of_words_mapped_to_UNK]\n",
    "print(words_mapped_to_UNK[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the estimated IBM1 translation probabilities from the lexicon file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(lexicon_file, encoding='utf8') as f:\n",
    "    dictionary_lines = f.read().splitlines()\n",
    "\n",
    "\n",
    "translation_probs_ZH_to_EN = {}\n",
    "translation_probs_EN_to_ZH = {}\n",
    "\n",
    "for line in dictionary_lines:\n",
    "    entries = line.split(' ')\n",
    "    if (entries[0] not in translation_probs_ZH_to_EN and entries[0] not in words_mapped_to_UNK):\n",
    "        translation_probs_ZH_to_EN[entries[0]] = {}\n",
    "    if (entries[1] not in translation_probs_EN_to_ZH):\n",
    "        translation_probs_EN_to_ZH[entries[1]] = {}\n",
    "    if (entries[2] != \"NA\" and entries[0] not in words_mapped_to_UNK):\n",
    "        translation_probs_ZH_to_EN[entries[0]][entries[1]] = float(entries[2])\n",
    "    if (entries[3] != \"NA\"):\n",
    "        translation_probs_EN_to_ZH[entries[1]][entries[0]] = float(entries[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0052879005670547485\n",
      "['wiper', 'au', 'shin', 'produced', 'confirm', 'spouses', 'two-year', 'climbs', 'ache', 'defective', 'norm', 'pants', 'oranges', 'shaded', 'gallons', 'souvenirs', 'slim', 'presently', 'magnifying', 'finest']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'服侍'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-02b28984a231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_probs_EN_to_ZH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<NULL>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"在\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_probs_ZH_to_EN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<NULL>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NOT ORDERED BY MOST LIKELY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_probs_ZH_to_EN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_mapped_to_UNK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NOT ORDERED BY MOST LIKELY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '服侍'"
     ]
    }
   ],
   "source": [
    "# TEST CELL\n",
    "\n",
    "#translation_probs_ZH_to_EN[\"<NULL>\"]\n",
    "print(translation_probs_EN_to_ZH[\"<NULL>\"][\"在\"])\n",
    "print(list(translation_probs_ZH_to_EN[\"<NULL>\"])[:20]) # NOT ORDERED BY MOST LIKELY\n",
    "print(list(translation_probs_ZH_to_EN[words_mapped_to_UNK[0]])[:20]) # NOT ORDERED BY MOST LIKELY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select only the top_n most likely translations for each source word (and the top_n_NULL most likely translations for the NULL symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_n_translation_probs_ZH_to_EN = {}\n",
    "\n",
    "for entry in translation_probs_ZH_to_EN:\n",
    "    if entry != '<NULL>':\n",
    "        entry_key = entry\n",
    "        new_entry = dict(sorted(translation_probs_ZH_to_EN[entry].items(), key=operator.itemgetter(1), reverse=True)[:top_n])\n",
    "    else:\n",
    "        #entry_key = '-EPS-'\n",
    "        entry_key = '<NULL>'\n",
    "        new_entry = dict(sorted(translation_probs_ZH_to_EN[entry].items(), key=operator.itemgetter(1), reverse=True)[:top_n_NULL])\n",
    "    \n",
    "    top_n_translation_probs_ZH_to_EN[entry_key] = new_entry\n",
    "    \n",
    "    \n",
    "\n",
    "top_n_translation_probs_EN_to_ZH = {}\n",
    "\n",
    "for entry in translation_probs_EN_to_ZH:\n",
    "    if entry != '<NULL>':\n",
    "        entry_key = entry\n",
    "        new_entry = dict(sorted(translation_probs_EN_to_ZH[entry].items(), key=operator.itemgetter(1), reverse=True)[:top_n])\n",
    "    else:\n",
    "        #entry_key = '-EPS-'\n",
    "        entry_key = '<NULL>'\n",
    "        new_entry = dict(sorted(translation_probs_EN_to_ZH[entry].items(), key=operator.itemgetter(1), reverse=True)[:top_n_NULL])\n",
    "    \n",
    "    top_n_translation_probs_EN_to_ZH[entry_key] = new_entry\n",
    "    \n",
    "    \n",
    "pickle.dump(top_n_translation_probs_ZH_to_EN, open('data/top' + str(top_n) + '_topNULL' + str(top_n_NULL) +\n",
    "                                                   '_%unseen' + str(percentage_of_one_occurence_words_to_UNK) +\n",
    "                                                   '_translation_probs_ZH_to_EN.mem', 'wb'))\n",
    "pickle.dump(top_n_translation_probs_EN_to_ZH, open('data/top' + str(top_n) + '_topNULL' + str(top_n_NULL) + '_translation_probs_EN_to_ZH.mem', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selects only those paired sentences whose target sentence length is smaller than the specified length constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining sentence pairs given target sentence length constraint: 33255\n"
     ]
    }
   ],
   "source": [
    "size_constrained_paired_sentences = []\n",
    "\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    chinese_side = pair_sentence[0].split(' ')\n",
    "    english_side = pair_sentence[1].split(' ')\n",
    "    if (len(english_side) <= sentence_size_value_constraint and len(chinese_side) <= sentence_size_value_constraint):\n",
    "        size_constrained_paired_sentences.append(pair)\n",
    "        \n",
    "print(\"Number of remaining sentence pairs given target sentence length constraint: \" +\n",
    "      str(len(size_constrained_paired_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to -UNK- of unobserved types in the constrained top-n lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to UNK... 100.0% sentences processed so far."
     ]
    }
   ],
   "source": [
    "if UNK:\n",
    "    size_and_UNK_constrained_paired_sentences = []\n",
    "\n",
    "    number_of_sentences = len(size_constrained_paired_sentences)\n",
    "\n",
    "    for i, pair in enumerate(size_constrained_paired_sentences):\n",
    "        pair_sentence = pair.split(' ||| ')\n",
    "        chinese_side = pair_sentence[0].split(' ')\n",
    "        english_side = pair_sentence[1].split(' ')\n",
    "\n",
    "\n",
    "        english_side_set = set(english_side)\n",
    "\n",
    "        chinese_UNK_sentence = []\n",
    "        english_UNK_sentence = []\n",
    "\n",
    "        \n",
    "        set_of_possible_translations = []\n",
    "        for key in top_n_translation_probs_ZH_to_EN['<NULL>']:\n",
    "            set_of_possible_translations.append(key)\n",
    "            \n",
    "        for chinese_word in chinese_side:\n",
    "            if chinese_word not in words_mapped_to_UNK:\n",
    "                for key in top_n_translation_probs_ZH_to_EN[chinese_word]:\n",
    "                    set_of_possible_translations.append(key)\n",
    "                chinese_UNK_sentence.append(chinese_word)\n",
    "            else:\n",
    "                chinese_UNK_sentence.append('-UNK-')\n",
    "\n",
    "\n",
    "        set_of_possible_translations = set(set_of_possible_translations)\n",
    "\n",
    "        for english_word in english_side:\n",
    "            if(english_word not in set_of_possible_translations):\n",
    "                english_UNK_sentence.append('-UNK-')\n",
    "            else:\n",
    "                english_UNK_sentence.append(english_word)\n",
    "\n",
    "        new_pair = chinese_UNK_sentence\n",
    "        new_pair.append('|||')\n",
    "        new_pair = new_pair + english_UNK_sentence\n",
    "\n",
    "        new_pair = ' '.join(str(e) for e in new_pair)\n",
    "        size_and_UNK_constrained_paired_sentences.append(new_pair)\n",
    "\n",
    "        if (i % 10 == 0 or i + 1 == number_of_sentences):\n",
    "            print('\\r' + 'Converting to UNK... ' + str(100.0*(i+1)/number_of_sentences) + '% sentences processed so far.', end='')\n",
    "            \n",
    "else:\n",
    "    print('UNK option was not selected! No conversion to UNK performed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determines the partition limits of the training data for dividing the original data file into 3 similar parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if UNK:\n",
    "    paired_sentences_to_use = size_and_UNK_constrained_paired_sentences\n",
    "else:\n",
    "    paired_sentences_to_use = size_constrained_paired_sentences\n",
    "    \n",
    "number_of_training_examples = len(paired_sentences_to_use)\n",
    "\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    number_of_subset_training_examples = number_of_training_examples/3\n",
    "else:\n",
    "    number_of_subset_training_examples12 = number_of_training_examples/3\n",
    "    number_of_subset_training_examples3 = number_of_training_examples - 2*number_of_subset_training_examples12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates 3 new training data files, which are basically disjoint sets of the original file and where their union is the entire training data (on constrained length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if UNK:\n",
    "    unk = '_UNK'\n",
    "else:\n",
    "    unk = '_noUNK'\n",
    "\n",
    "low_limit = 0\n",
    "\n",
    "# First Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = number_of_subset_training_examples12\n",
    "\n",
    "f = open('data/training_subset1_size' + str(sentence_size_value_constraint)+\n",
    "         '_top'+str(top_n)+ '_topNULL'+str(top_n_NULL)+'_%unseen'+\n",
    "         str(percentage_of_one_occurence_words_to_UNK)+unk+'.zh-en', 'w', encoding='utf8')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(paired_sentences_to_use[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Second Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = 2*number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = 2*number_of_subset_training_examples12\n",
    "\n",
    "f = open('data/training_subset2_size' + str(sentence_size_value_constraint)+\n",
    "         '_top'+str(top_n)+ '_topNULL'+str(top_n_NULL)+'_%unseen'+\n",
    "         str(percentage_of_one_occurence_words_to_UNK)+unk+'.zh-en', 'w', encoding='utf8')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(paired_sentences_to_use[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Third Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = 3*number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = 2*number_of_subset_training_examples12 + number_of_subset_training_examples3\n",
    "\n",
    "f = open('data/training_subset3_size' + str(sentence_size_value_constraint)+\n",
    "         '_top'+str(top_n)+ '_topNULL'+str(top_n_NULL)+'_%unseen'+\n",
    "         str(percentage_of_one_occurence_words_to_UNK)+unk+'.zh-en', 'w', encoding='utf8')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(paired_sentences_to_use[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the original training set into seperate chinese and english corpus (used for word2vec, maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chinese_corpus = []\n",
    "english_corpus = []\n",
    "\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    chinese_corpus.append(pair_sentence[0])\n",
    "    english_corpus.append(pair_sentence[1])\n",
    "    \n",
    "    \n",
    "f = open('data/chinese.zh-en', 'w')\n",
    "for entry in chinese_corpus:\n",
    "    f.write(entry)\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open('data/english.zh-en', 'w')\n",
    "for entry in english_corpus:\n",
    "    f.write(entry)\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dev Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_files = [\"datamap/references_val/dev1.zh-en\", \"datamap/references_test/dev2.zh-en\"]\n",
    "references = [\"datamap/references_val/reference\", \"datamap/references_test/reference\"]\n",
    "top_n_translation_probs_ZH_to_EN_path = \"datamap/top5_topNULL2_%unseen0.8_translation_probs_ZH_to_EN.mem\"\n",
    "\n",
    "paired_sentences_dev = [0, 0]\n",
    "chinese_sentences = [0, 0]\n",
    "chinese_sentences_path = [\"datamap/references_val/chinese_val.zh\", \"datamap/references_test/chinese_test.zh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(top_n_translation_probs_ZH_to_EN_path, 'rb') as pickle_file:\n",
    "    top_n_translation_probs_ZH_to_EN = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 很 讨厌 青椒 。 ||| i hate green peppers . ||| i despise green peppers . ||| i can 't stand green peppers . ||| i just plain hate green peppers . ||| i detest green peppers . ||| i can 't stand green peppers . ||| green peppers make me sick . ||| i can 't stand green peppers . ||| i don 't like green peppers , at all . ||| green peppers are something i simply can 't stand . ||| i don 't like green peppers . ||| i dislike green peppers . ||| i 'm not keen on green peppers . ||| i dislike green peppers . ||| i don 't like green peppers . ||| green peppers taste terrible .\n",
      "16\n",
      "16\n",
      "500\n",
      "506\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i, file in enumerate(dev_files):\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        paired_sentences_dev[i] = f.read().splitlines()\n",
    "\n",
    "\n",
    "print(paired_sentences_dev[0][12])\n",
    "max_number_of_possible_translations = [0, 0]\n",
    "for l, dev_set in enumerate(paired_sentences_dev):\n",
    "    for i, translation in enumerate(dev_set):\n",
    "        dev_set[i] = translation.split(' ||| ')\n",
    "        for j, sentence in enumerate(dev_set[i]):\n",
    "            if j > max_number_of_possible_translations[l]:\n",
    "                max_number_of_possible_translations[l] = j\n",
    "            if (j == 0):\n",
    "                sentence_set_of_possible_translations = []\n",
    "            dev_set[i][j] = sentence.split(' ')\n",
    "            for k, word in enumerate(dev_set[i][j]):\n",
    "                if (j == 0):\n",
    "                    for key in top_n_translation_probs_ZH_to_EN['<NULL>']:\n",
    "                        sentence_set_of_possible_translations.append(key)\n",
    "                    if word not in top_n_translation_probs_ZH_to_EN:\n",
    "                        dev_set[i][j][k] = \"-UNK-\"\n",
    "                    else:\n",
    "                        for key in top_n_translation_probs_ZH_to_EN[word]:\n",
    "                            sentence_set_of_possible_translations.append(key)\n",
    "                    sentence_set_of_possible_translations = list(set(sentence_set_of_possible_translations))\n",
    "                else:\n",
    "                    if word not in sentence_set_of_possible_translations:\n",
    "                        dev_set[i][j][k] = \"-UNK-\"\n",
    "                        \n",
    "            #if (l == 0 and i == 12 and j == 0):\n",
    "            #    print(sentence_set_of_possible_translations)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "for l, dev_set in enumerate(paired_sentences_dev):\n",
    "    for i, translation in enumerate(dev_set):\n",
    "        for j, sentence in enumerate(dev_set[i]):\n",
    "            if j == 0:\n",
    "                filename = chinese_sentences_path[l]\n",
    "                \n",
    "                if os.path.exists(filename):\n",
    "                    os.remove(filename)\n",
    "                continue\n",
    "            \n",
    "            filename = references[l] + str(j)\n",
    "\n",
    "            if os.path.exists(filename):\n",
    "                os.remove(filename)\n",
    "                \n",
    "        if j < max_number_of_possible_translations[l]:\n",
    "            for k in range(max_number_of_possible_translations[l]-j-1):\n",
    "                filename = references[l] + str(j+k+1)\n",
    "\n",
    "                if os.path.exists(filename):\n",
    "                    os.remove(filename)\n",
    "                    \n",
    "                    \n",
    "\n",
    "            \n",
    "for l, dev_set in enumerate(paired_sentences_dev):\n",
    "    for i, translation in enumerate(dev_set):\n",
    "        for j, sentence in enumerate(dev_set[i]):\n",
    "            if j == 0:\n",
    "                chinese_sentence = ' '.join(str(word) for word in sentence) + '\\n'\n",
    "                filename = chinese_sentences_path[l]\n",
    "                \n",
    "                if os.path.exists(filename):\n",
    "                    append_write = 'a' # append if already exists\n",
    "                else:\n",
    "                    append_write = 'w' # make a new file if not\n",
    "\n",
    "                with open(filename, append_write) as reference_file:\n",
    "                    reference_file.write(chinese_sentence)\n",
    "                continue\n",
    "            \n",
    "            reference_translation = ' '.join(str(word) for word in sentence) + '\\n'\n",
    "            \n",
    "            filename = references[l] + str(j)\n",
    "\n",
    "            if os.path.exists(filename):\n",
    "                append_write = 'a' # append if already exists\n",
    "            else:\n",
    "                append_write = 'w' # make a new file if not\n",
    "\n",
    "            with open(filename, append_write) as reference_file:\n",
    "                reference_file.write(reference_translation)\n",
    "                \n",
    "        if j < max_number_of_possible_translations[l]:\n",
    "            for k in range(max_number_of_possible_translations[l]-j-1):\n",
    "                reference_translation = '\\n'\n",
    "            \n",
    "                filename = references[l] + str(j+k+1)\n",
    "\n",
    "                if os.path.exists(filename):\n",
    "                    append_write = 'a' # append if already exists\n",
    "                else:\n",
    "                    append_write = 'w' # make a new file if not\n",
    "\n",
    "                with open(filename, append_write) as reference_file:\n",
    "                    reference_file.write(reference_translation)\n",
    "\n",
    "    \n",
    "print(max_number_of_possible_translations[0])\n",
    "print(max_number_of_possible_translations[1])\n",
    "\n",
    "print(len(paired_sentences_dev[0]))\n",
    "print(len(paired_sentences_dev[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['我', '很', '讨厌', '-UNK-', '。'], ['i', 'hate', '-UNK-', '-UNK-', '.'], ['i', '-UNK-', '-UNK-', '-UNK-', '.'], ['i', '-UNK-', \"'t\", '-UNK-', '-UNK-', '-UNK-', '.'], ['i', '-UNK-', '-UNK-', 'hate', '-UNK-', '-UNK-', '.'], ['i', '-UNK-', '-UNK-', '-UNK-', '.'], ['i', '-UNK-', \"'t\", '-UNK-', '-UNK-', '-UNK-', '.'], ['-UNK-', '-UNK-', '-UNK-', '-UNK-', '-UNK-', '.'], ['i', '-UNK-', \"'t\", '-UNK-', '-UNK-', '-UNK-', '.'], ['i', 'don', \"'t\", '-UNK-', '-UNK-', '-UNK-', ',', '-UNK-', '-UNK-', '.'], ['-UNK-', '-UNK-', '-UNK-', '-UNK-', 'i', '-UNK-', '-UNK-', \"'t\", '-UNK-', '.'], ['i', 'don', \"'t\", '-UNK-', '-UNK-', '-UNK-', '.'], ['i', 'dislike', '-UNK-', '-UNK-', '.'], ['i', \"'m\", 'not', '-UNK-', '-UNK-', '-UNK-', '-UNK-', '.'], ['i', 'dislike', '-UNK-', '-UNK-', '.'], ['i', 'don', \"'t\", '-UNK-', '-UNK-', '-UNK-', '.'], ['-UNK-', '-UNK-', '-UNK-', '-UNK-', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(paired_sentences_dev[0][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reference0 reference1 reference2\n",
      "program output: b'90.36 100.00 100.00 100.00 66.67\\n'\n",
      "90.36 100.00 100.00 100.00 66.67\n",
      "['90.36', '100.00', '100.00', '100.00', '66.67']\n",
      "[90.36, 100.0, 100.0, 100.0, 66.67]\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "number_of_reference_files = 3\n",
    "references = \"\"\n",
    "for i in range(number_of_reference_files):\n",
    "    references += \" reference\" + str(i)\n",
    "    \n",
    "print(references)\n",
    "\n",
    "output = subprocess.check_output(\"perl multi-bleu.perl -lc\" + references + \" < hypotheses\", shell=True)\n",
    "print(\"program output: \" + str(output))\n",
    "output = str(output)\n",
    "print(output[2:][:-3])\n",
    "output = output[2:][:-3].split(' ')\n",
    "print(output)\n",
    "for i, value in enumerate(output):\n",
    "    output[i] = float(value)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "elems = [1,2]\n",
    "\n",
    "for i in range(3):\n",
    "    for j, ele in enumerate(elems):\n",
    "        print(j)\n",
    "    if j < 4:\n",
    "        for k in range(4-j-1):\n",
    "            print(j+k+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
